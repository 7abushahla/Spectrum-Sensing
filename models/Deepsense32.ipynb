{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Conv2D, MaxPooling1D, LeakyReLU, Flatten, Input, Dropout, Lambda, Reshape, MaxPooling2D, ReLU\n",
    "from tensorflow.keras.models import Model\n",
    "import argparse\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.metrics import AUC, Precision, Recall\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "import logging\n",
    "import tempfile\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "import json\n",
    "import shutil  # For copying files\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress TensorFlow INFO, WARNING, and ERROR messages\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"GPU\" if tf.config.list_physical_devices('GPU') else \"CPU\"\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== Configuration Variables ==================\n",
    "# Experiment Configuration\n",
    "model_type = \"DeepSense\"      # Options: \"DeepSense\", \"ParallelCNN\"\n",
    "N = 32                        # Options: 128, 32\n",
    "training_type = \"normal\"      # Options: \"normal\", \"QAT\"\n",
    "\n",
    "# Reproducibility Settings\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Experiment Settings\n",
    "N_FOLDS = 5\n",
    "N_REPEATS = 3\n",
    "EPOCHS = 100\n",
    "BATCHSIZE = 256\n",
    "# =============================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== Naming Conventions ======================\n",
    "# Define naming patterns based on configuration\n",
    "metrics_filename = f\"{model_type}_{N}_{training_type}_metrics.json\"\n",
    "best_overall_model_filename = f\"{model_type}_{N}_{training_type}_best_overall_model.tflite\"  # Updated to .tflite\n",
    "tflite_model_filename_pattern = f\"{model_type}_{N}_{training_type}_fold_{{fold_number}}_model.tflite\"  # Dynamic naming per fold\n",
    "# ============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== Directory Setup =========================\n",
    "# Define base results directory\n",
    "base_results_dir = \"results\"\n",
    "\n",
    "# Construct the directory path\n",
    "experiment_dir = os.path.join(\n",
    "    base_results_dir,\n",
    "    model_type,\n",
    "    f\"N{N}\",\n",
    "    training_type\n",
    ")\n",
    "\n",
    "# Subdirectories for models, logs, metrics, and plots\n",
    "models_dir = os.path.join(experiment_dir, \"models\")\n",
    "logs_dir = os.path.join(experiment_dir, \"logs\")\n",
    "metrics_dir = os.path.join(experiment_dir, \"metrics\")\n",
    "plots_dir = os.path.join(metrics_dir, \"plots\")  # Directory to save plots\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "os.makedirs(metrics_dir, exist_ok=True)\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "# ============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== Custom F1 Metric ==========================\n",
    "def F1_Score(y_true, y_pred):\n",
    "    # Cast the y_true and y_pred to the right shape (binary for multi-label classification)\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred > 0.5, tf.float32)\n",
    "    \n",
    "    # Precision calculation\n",
    "    tp = tf.reduce_sum(tf.cast(y_true * y_pred, tf.float32), axis=0)\n",
    "    predicted_positives = tf.reduce_sum(tf.cast(y_pred, tf.float32), axis=0)\n",
    "    actual_positives = tf.reduce_sum(tf.cast(y_true, tf.float32), axis=0)\n",
    "\n",
    "    precision = tp / (predicted_positives + tf.keras.backend.epsilon())\n",
    "    recall = tp / (actual_positives + tf.keras.backend.epsilon())\n",
    "\n",
    "    # F1 calculation\n",
    "    f1 = 2 * ((precision * recall) / (precision + recall + tf.keras.backend.epsilon()))\n",
    "    \n",
    "    # Mean of F1 across all classes\n",
    "    return tf.reduce_mean(f1)\n",
    "# ============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (287971, 128, 2)\n",
      "Sample training data:\n",
      "[[-0.02026422 -0.05847325]\n",
      " [-0.02059992 -0.05859532]\n",
      " [-0.02026422 -0.05862584]\n",
      " [-0.01998955 -0.05847325]\n",
      " [-0.02038629 -0.0585648 ]]\n",
      "Labels shape: (287971, 4)\n",
      "Sample labels (first sample):\n",
      "[1 0 0 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (287971, 128, 2)\n",
      "Sample normalized training data:\n",
      "[[-0.06700726 -0.3899736 ]\n",
      " [-0.07814094 -0.39383325]\n",
      " [-0.06700726 -0.39479813]\n",
      " [-0.05789788 -0.3899736 ]\n",
      " [-0.07105588 -0.39286837]]\n"
     ]
    }
   ],
   "source": [
    "# ================== Data Loading =============================\n",
    "# Load training data from the .h5 file\n",
    "dset = h5py.File(\"./sdr_wifi_train_32buf.hdf5\", 'r')\n",
    "X = dset['X'][()]  # Shape: (287971, 32, 2)\n",
    "y = dset['y'][()]  # Shape: (287971,)\n",
    "\n",
    "print(f\"Training data shape: {X.shape}\")\n",
    "print(f\"Sample training data:\\n{X[0, :5, :2]}\")  # Display first 5 samples\n",
    "\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "print(f\"Sample labels (first sample):\\n{y[0]}\")  # Display first label\n",
    "\n",
    "# Normalize the training data\n",
    "# Calculate mean and standard deviation across the dataset for each channel\n",
    "mean = np.mean(X, axis=(0, 1))  # Mean for each channel\n",
    "std = np.std(X, axis=(0, 1))    # Standard deviation for each channel\n",
    "\n",
    "# Perform normalization: (X - mean) / std\n",
    "X_normalized = (X - mean) / std\n",
    "\n",
    "# Print normalized data for verification\n",
    "print(f\"Training data shape: {X_normalized.shape}\")\n",
    "print(f\"Sample normalized training data:\\n{X_normalized[0, :5, :]}\")\n",
    "# =============================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data shape: (31997, 128, 2)\n",
      "Sample testing data (first 10 samples):\n",
      "[[-0.01831104 -0.05761873]\n",
      " [-0.01953177 -0.05764925]\n",
      " [-0.019074   -0.05758822]\n",
      " [-0.01922659 -0.05917517]\n",
      " [-0.02035577 -0.05786288]]\n",
      "Test Labels shape: (31997, 4)\n",
      "Sample test labels (first sample):\n",
      "[1 1 0 0]\n",
      "Normalized testing data shape: (31997, 128, 2)\n",
      "Sample normalized testing data (first 10 samples):\n",
      "[[-0.00222953 -0.36295623]\n",
      " [-0.04271558 -0.3639211 ]\n",
      " [-0.02753333 -0.36199135]\n",
      " [-0.03259408 -0.41216648]\n",
      " [-0.0700437  -0.3706755 ]]\n"
     ]
    }
   ],
   "source": [
    "# ================== Load Testing Data ========================\n",
    "# Load testing data from the .h5 file\n",
    "test_dset = h5py.File(\"./sdr_wifi_test_32buf.hdf5\", 'r')\n",
    "X_test = test_dset['X'][()]\n",
    "y_test = test_dset['y'][()]\n",
    "\n",
    "print(f\"Testing data shape: {X_test.shape}\")\n",
    "print(f\"Sample testing data (first 10 samples):\\n{X_test[0, :5, :]}\")\n",
    "\n",
    "print(f\"Test Labels shape: {y_test.shape}\")\n",
    "print(f\"Sample test labels (first sample):\\n{y_test[0]}\")\n",
    "\n",
    "# Normalize the test data using training mean and std\n",
    "X_test_normalized = (X_test - mean) / std\n",
    "\n",
    "print(f\"Normalized testing data shape: {X_test_normalized.shape}\")\n",
    "print(f\"Sample normalized testing data (first 10 samples):\\n{X_test_normalized[0, :5, :]}\")\n",
    "# ============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Fold 1/2\n",
      "Train shape: (143985, 128, 2), Validation shape: (143986, 128, 2)\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7/563 [..............................] - ETA: 9s - loss: 0.6856 - Precision: 0.5221 - Recall: 0.8175 - F1_Score: 0.6199 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 20:30:20.596673: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] remapper failed: INVALID_ARGUMENT: Mutation::Apply error: fanout 'gradient_tape/model_14/leaky_re_lu_73/LeakyRelu/LeakyReluGrad' exist for missing node 'model_14/conv4/BiasAdd'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "562/563 [============================>.] - ETA: 0s - loss: 0.3034 - Precision: 0.8987 - Recall: 0.7747 - F1_Score: 0.8291\n",
      "Epoch 1: val_F1_Score improved from -inf to 0.93513, saving model to results/DeepSense/N128/normal/models/DeepSense_128_normal_fold_1_best_model.h5\n",
      "563/563 [==============================] - 12s 20ms/step - loss: 0.3033 - Precision: 0.8988 - Recall: 0.7748 - F1_Score: 0.8294 - val_loss: 0.1524 - val_Precision: 0.9886 - val_Recall: 0.8928 - val_F1_Score: 0.9351 - lr: 0.0010\n",
      "Epoch 2/3\n",
      "  4/563 [..............................] - ETA: 9s - loss: 0.1887 - Precision: 0.9716 - Recall: 0.9004 - F1_Score: 0.9338"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563/563 [==============================] - ETA: 0s - loss: 0.1087 - Precision: 0.9773 - Recall: 0.9544 - F1_Score: 0.9650\n",
      "Epoch 2: val_F1_Score improved from 0.93513 to 0.98695, saving model to results/DeepSense/N128/normal/models/DeepSense_128_normal_fold_1_best_model.h5\n",
      "563/563 [==============================] - 11s 19ms/step - loss: 0.1087 - Precision: 0.9773 - Recall: 0.9544 - F1_Score: 0.9650 - val_loss: 0.0596 - val_Precision: 0.9863 - val_Recall: 0.9876 - val_F1_Score: 0.9869 - lr: 0.0010\n",
      "Epoch 3/3\n",
      "561/563 [============================>.] - ETA: 0s - loss: 0.0672 - Precision: 0.9844 - Recall: 0.9730 - F1_Score: 0.9784\n",
      "Epoch 3: val_F1_Score did not improve from 0.98695\n",
      "563/563 [==============================] - 11s 20ms/step - loss: 0.0672 - Precision: 0.9844 - Recall: 0.9731 - F1_Score: 0.9784 - val_loss: 0.0488 - val_Precision: 0.9951 - val_Recall: 0.9708 - val_F1_Score: 0.9823 - lr: 0.0010\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpgfkzmie9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpgfkzmie9/assets\n",
      "2025-01-27 20:30:55.511804: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2025-01-27 20:30:55.511854: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2025-01-27 20:30:55.512061: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpgfkzmie9\n",
      "2025-01-27 20:30:55.513519: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2025-01-27 20:30:55.513536: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmpgfkzmie9\n",
      "2025-01-27 20:30:55.517524: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2025-01-27 20:30:55.569349: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmpgfkzmie9\n",
      "2025-01-27 20:30:55.587244: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 75183 microseconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted TFLite model saved to results/DeepSense/N128/normal/models/DeepSense_128_normal_fold_1_model.tflite\n",
      "Fold 1 - Test Precision: 0.9858, Recall: 0.9871, F1-Score: 0.9864\n",
      "New best overall model found in Fold 1 with Validation F1-Score: 0.9869\n",
      "\n",
      "Starting Fold 2/2\n",
      "Train shape: (143986, 128, 2), Validation shape: (143985, 128, 2)\n",
      "Epoch 1/3\n",
      "560/563 [============================>.] - ETA: 0s - loss: 0.2831 - Precision: 0.8992 - Recall: 0.7984 - F1_Score: 0.8432\n",
      "Epoch 1: val_F1_Score improved from -inf to 0.94611, saving model to results/DeepSense/N128/normal/models/DeepSense_128_normal_fold_2_best_model.h5\n",
      "563/563 [==============================] - 12s 21ms/step - loss: 0.2828 - Precision: 0.8994 - Recall: 0.7990 - F1_Score: 0.8437 - val_loss: 0.1361 - val_Precision: 0.9752 - val_Recall: 0.9218 - val_F1_Score: 0.9461 - lr: 0.0010\n",
      "Epoch 2/3\n",
      "  3/563 [..............................] - ETA: 16s - loss: 0.1585 - Precision: 0.9717 - Recall: 0.9149 - F1_Score: 0.9405"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560/563 [============================>.] - ETA: 0s - loss: 0.1044 - Precision: 0.9782 - Recall: 0.9542 - F1_Score: 0.9656\n",
      "Epoch 2: val_F1_Score improved from 0.94611 to 0.98514, saving model to results/DeepSense/N128/normal/models/DeepSense_128_normal_fold_2_best_model.h5\n",
      "563/563 [==============================] - 11s 20ms/step - loss: 0.1044 - Precision: 0.9783 - Recall: 0.9542 - F1_Score: 0.9656 - val_loss: 0.0624 - val_Precision: 0.9776 - val_Recall: 0.9926 - val_F1_Score: 0.9851 - lr: 0.0010\n",
      "Epoch 3/3\n",
      "559/563 [============================>.] - ETA: 0s - loss: 0.0627 - Precision: 0.9852 - Recall: 0.9743 - F1_Score: 0.9795\n",
      "Epoch 3: val_F1_Score improved from 0.98514 to 0.98536, saving model to results/DeepSense/N128/normal/models/DeepSense_128_normal_fold_2_best_model.h5\n",
      "563/563 [==============================] - 12s 22ms/step - loss: 0.0626 - Precision: 0.9853 - Recall: 0.9742 - F1_Score: 0.9795 - val_loss: 0.0487 - val_Precision: 0.9778 - val_Recall: 0.9926 - val_F1_Score: 0.9854 - lr: 0.0010\n",
      "INFO:tensorflow:Assets written to: /tmp/tmphn0xdv5x/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmphn0xdv5x/assets\n",
      "2025-01-27 20:31:35.493191: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2025-01-27 20:31:35.493242: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2025-01-27 20:31:35.493444: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmphn0xdv5x\n",
      "2025-01-27 20:31:35.498016: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2025-01-27 20:31:35.498054: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmphn0xdv5x\n",
      "2025-01-27 20:31:35.507806: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2025-01-27 20:31:35.576480: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmphn0xdv5x\n",
      "2025-01-27 20:31:35.611770: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 118325 microseconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted TFLite model saved to results/DeepSense/N128/normal/models/DeepSense_128_normal_fold_2_model.tflite\n",
      "Fold 2 - Test Precision: 0.9793, Recall: 0.9921, F1-Score: 0.9854\n"
     ]
    }
   ],
   "source": [
    "# ================== Cross-Validation Setup ===================\n",
    "# Define cross-validation parameters\n",
    "n_splits = N_FOLDS\n",
    "n_repeats = N_REPEATS\n",
    "random_state = SEED\n",
    "\n",
    "# Initialize RepeatedKFold\n",
    "rkf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=random_state)\n",
    "\n",
    "# Initialize lists to store aggregated metrics\n",
    "validation_precisions = []\n",
    "validation_recalls = []\n",
    "validation_f1_scores = []\n",
    "\n",
    "test_precisions = []\n",
    "test_recalls = []\n",
    "test_f1_scores = []\n",
    "\n",
    "# Variables to track the best overall model based on validation F1-score\n",
    "best_overall_f1 = -1\n",
    "best_overall_test_f1 = -1  # Initialize to track test F1-score for the best model\n",
    "best_overall_model_path = os.path.join(models_dir, best_overall_model_filename)\n",
    "\n",
    "# Variable to store the training history of the best overall model\n",
    "best_overall_history = None\n",
    "\n",
    "# Define TFLite model path pattern per fold\n",
    "tflite_model_filename_pattern = f\"{model_type}_{N}_{training_type}_fold_{{fold_number}}_model.tflite\"\n",
    "\n",
    "# Fold counter\n",
    "total_folds = n_splits * n_repeats\n",
    "fold_number = 1\n",
    "# =============================================================\n",
    "\n",
    "# ================== Training Loop =============================\n",
    "for train_index, val_index in rkf.split(X_normalized):\n",
    "    print(f\"\\nStarting Fold {fold_number}/{total_folds}\")\n",
    "    \n",
    "    # Split the data into training and validation sets for this fold\n",
    "    X_train_fold, X_val_fold = X_normalized[train_index], X_normalized[val_index]\n",
    "    y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "    \n",
    "    print(f\"Train shape: {X_train_fold.shape}, Validation shape: {X_val_fold.shape}\")\n",
    "    \n",
    "    # ================== Model Building ========================\n",
    "    # Define model parameters\n",
    "    n_classes = y.shape[1]       # number of classes for multi-label classification\n",
    "    dim = X_normalized.shape[1]  # Number of I/Q samples being taken as input\n",
    "    n_channels = X_normalized.shape[2]  # Number of channels (I and Q)\n",
    "    \n",
    "    # Build the model\n",
    "    inputs = Input(shape=(dim, n_channels), dtype=tf.float32, name='input_layer')\n",
    "    \n",
    "    # Reshape input to fit Conv2D requirements: (1, dim, n_channels)\n",
    "    reshaped_inputs = Reshape((1, dim, n_channels))(inputs)\n",
    "    \n",
    "    # First Conv stack\n",
    "    x = Conv2D(16, (1, 3), name='conv1')(reshaped_inputs)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = Conv2D(16, (1, 3), name='conv2')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = MaxPooling2D(pool_size=(1, 2), strides=(1, 2), name='pool1')(x)\n",
    "    x = Dropout(0.3)(x)  # Dropout added after the first stack\n",
    "    \n",
    "    # Second Conv stack\n",
    "    x = Conv2D(32, (1, 5), name='conv3')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = Conv2D(32, (1, 5), name='conv4')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = MaxPooling2D(pool_size=(1, 2), strides=(1, 2), name='pool2')(x)\n",
    "    x = Dropout(0.3)(x)  # Dropout added after the second stack\n",
    "    \n",
    "    # Fully connected layer\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64, name='dense1')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = Dense(n_classes, activation='sigmoid', name='out')(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    # =============================================================\n",
    "    \n",
    "    # ================== Model Compilation =====================\n",
    "    # Compile the model\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    \n",
    "    model.compile(\n",
    "        loss='binary_crossentropy', \n",
    "        optimizer=adam, \n",
    "        metrics=[\n",
    "            tf.keras.metrics.Precision(name='Precision'), \n",
    "            tf.keras.metrics.Recall(name='Recall'), \n",
    "            F1_Score\n",
    "        ]\n",
    "    )\n",
    "    # =============================================================\n",
    "    \n",
    "    # ================== Callbacks Setup =======================\n",
    "    # Define a unique directory for TensorBoard logs per fold\n",
    "    fold_log_dir = os.path.join(\n",
    "        logs_dir, \n",
    "        f\"fold_{fold_number}_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    )\n",
    "    tensorboard_callback = TensorBoard(log_dir=fold_log_dir, histogram_freq=1)\n",
    "    \n",
    "    # Define ModelCheckpoint to save the best model based on validation F1-score\n",
    "    checkpoint_filename = f\"{model_type}_{N}_{training_type}_fold_{fold_number}_best_model.h5\"\n",
    "    checkpoint_path = os.path.join(models_dir, checkpoint_filename)\n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        monitor='val_F1_Score',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Other callbacks\n",
    "    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
    "    # =============================================================\n",
    "    \n",
    "    # ================== Model Training =========================\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        x=X_train_fold, \n",
    "        y=y_train_fold, \n",
    "        validation_data=(X_val_fold, y_val_fold),\n",
    "        batch_size=BATCHSIZE, \n",
    "        epochs=EPOCHS, \n",
    "        verbose=1, \n",
    "        shuffle=True,  \n",
    "        callbacks=[lr_scheduler, early_stopping, model_checkpoint, tensorboard_callback]\n",
    "    )\n",
    "    # =============================================================\n",
    "    \n",
    "    # ================== Model Evaluation ========================\n",
    "    # Load the best model for this fold\n",
    "    best_model_fold = load_model(\n",
    "        checkpoint_path, \n",
    "        custom_objects={'F1_Score': F1_Score}\n",
    "    )\n",
    "    \n",
    "    # Convert the best model to TFLite\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(best_model_fold)\n",
    "    try:\n",
    "        tflite_model = converter.convert()\n",
    "        # Define the TFLite model path for this fold\n",
    "        current_fold_tflite_path = os.path.join(\n",
    "            models_dir, \n",
    "            tflite_model_filename_pattern.format(fold_number=fold_number)\n",
    "        )\n",
    "        # Save the TFLite model\n",
    "        with open(current_fold_tflite_path, \"wb\") as f:\n",
    "            f.write(tflite_model)\n",
    "        print(f\"Converted TFLite model saved to {current_fold_tflite_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Fold {fold_number} - TFLite conversion failed with error: {e}\")\n",
    "        current_fold_tflite_path = None\n",
    "    \n",
    "    if current_fold_tflite_path and os.path.exists(current_fold_tflite_path):\n",
    "        # Load the TFLite model and allocate tensors\n",
    "        interpreter = tf.lite.Interpreter(model_path=current_fold_tflite_path)\n",
    "        interpreter.allocate_tensors()\n",
    "        \n",
    "        # Get input and output details\n",
    "        input_details = interpreter.get_input_details()\n",
    "        output_details = interpreter.get_output_details()\n",
    "        \n",
    "        # Function to run inference with TFLite\n",
    "        def run_inference_tflite(interpreter, X):\n",
    "            # Assuming single input\n",
    "            input_index = input_details[0]['index']\n",
    "            output_index = output_details[0]['index']\n",
    "            \n",
    "            # Ensure X has the right shape (batch_size, dim, n_channels)\n",
    "            if len(X.shape) == 2:\n",
    "                X = np.expand_dims(X, axis=0)\n",
    "            \n",
    "            interpreter.set_tensor(input_index, X.astype(np.float32))\n",
    "            interpreter.invoke()\n",
    "            output = interpreter.get_tensor(output_index)\n",
    "            return output\n",
    "        \n",
    "        # Evaluate the model on the test set\n",
    "        y_pred = []\n",
    "        batch_size = 1  # Adjust as needed\n",
    "        \n",
    "        for i in range(0, len(X_test_normalized), batch_size):\n",
    "            X_batch = X_test_normalized[i:i + batch_size]\n",
    "            predictions = run_inference_tflite(interpreter, X_batch)\n",
    "            y_pred.append(predictions)\n",
    "        \n",
    "        # Concatenate all predictions\n",
    "        y_pred = np.concatenate(y_pred, axis=0)\n",
    "        \n",
    "        # Convert probabilities to binary predictions\n",
    "        y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "        \n",
    "        # Compute Precision, Recall, and F1-Score using scikit-learn\n",
    "        precision = precision_score(y_test, y_pred_binary, average='macro', zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred_binary, average='macro', zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred_binary, average='macro', zero_division=0)\n",
    "        \n",
    "        print(f\"Fold {fold_number} - Test Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
    "        \n",
    "        # Append test metrics to the aggregated lists\n",
    "        test_precisions.append(precision)\n",
    "        test_recalls.append(recall)\n",
    "        test_f1_scores.append(f1)\n",
    "    else:\n",
    "        print(f\"Fold {fold_number} - Skipping test evaluation due to TFLite conversion failure.\")\n",
    "    \n",
    "    # =============================================================\n",
    "    \n",
    "    # ================== Best Overall Model ======================\n",
    "    # Evaluate validation F1-score from training history\n",
    "    if 'history' in locals():\n",
    "        if 'val_F1_Score' in history.history:\n",
    "            val_f1 = max(history.history['val_F1_Score'])\n",
    "            # Append validation metrics\n",
    "            val_precision = max(history.history['Precision'])\n",
    "            val_recall = max(history.history['Recall'])\n",
    "            validation_precisions.append(val_precision)\n",
    "            validation_recalls.append(val_recall)\n",
    "            validation_f1_scores.append(val_f1)\n",
    "        else:\n",
    "            print(f\"Fold {fold_number} - 'val_F1_Score' not found in history.\")\n",
    "            val_f1 = -1  # Assign a default value or handle appropriately\n",
    "    else:\n",
    "        print(f\"Fold {fold_number} - 'history' object not found.\")\n",
    "        val_f1 = -1  # Assign a default value or handle appropriately\n",
    "    \n",
    "    # Check if this fold has the best validation F1-score\n",
    "    if val_f1 > best_overall_f1 and current_fold_tflite_path:\n",
    "        best_overall_f1 = val_f1\n",
    "        best_overall_test_f1 = f1  # Update test F1-score for the best model\n",
    "        best_overall_history = history  # Store the training history of the best model\n",
    "        \n",
    "        # Copy the current fold's TFLite model to the best_overall_model_path\n",
    "        shutil.copy(current_fold_tflite_path, best_overall_model_path)\n",
    "        print(f\"New best overall model found in Fold {fold_number} with Validation F1-Score: {val_f1:.4f}\")\n",
    "    elif val_f1 > best_overall_f1 and not current_fold_tflite_path:\n",
    "        print(f\"Fold {fold_number} has a better validation F1-Score ({val_f1:.4f}) but TFLite conversion failed. Not updating the best model.\")\n",
    "    # =============================================================\n",
    "    \n",
    "    fold_number += 1\n",
    "# ============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final aggregated metrics have been saved to 'results/DeepSense/N128/normal/metrics/DeepSense_128_normal_metrics.json'.\n",
      "Best overall TFLite model saved at: 'results/DeepSense/N128/normal/models/DeepSense_128_normal_best_overall_model.tflite'\n",
      "\n",
      "Generating plots for the best overall model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plots for the best overall model have been saved to 'results/DeepSense/N128/normal/metrics/plots'.\n"
     ]
    }
   ],
   "source": [
    "# ================== Aggregating Metrics ======================\n",
    "# Compute average and standard deviation for validation metrics\n",
    "avg_val_precision = np.mean(validation_precisions) if validation_precisions else 0\n",
    "std_val_precision = np.std(validation_precisions) if validation_precisions else 0\n",
    "\n",
    "avg_val_recall = np.mean(validation_recalls) if validation_recalls else 0\n",
    "std_val_recall = np.std(validation_recalls) if validation_recalls else 0\n",
    "\n",
    "avg_val_f1 = np.mean(validation_f1_scores) if validation_f1_scores else 0\n",
    "std_val_f1 = np.std(validation_f1_scores) if validation_f1_scores else 0\n",
    "\n",
    "# Compute average and standard deviation for test metrics\n",
    "avg_test_precision = np.mean(test_precisions) if test_precisions else 0\n",
    "std_test_precision = np.std(test_precisions) if test_precisions else 0\n",
    "\n",
    "avg_test_recall = np.mean(test_recalls) if test_recalls else 0\n",
    "std_test_recall = np.std(test_recalls) if test_recalls else 0\n",
    "\n",
    "avg_test_f1 = np.mean(test_f1_scores) if test_f1_scores else 0\n",
    "std_test_f1 = np.std(test_f1_scores) if test_f1_scores else 0\n",
    "# =============================================================\n",
    "\n",
    "# ================== Metrics Summary ==========================\n",
    "# Prepare the metrics summary\n",
    "metrics_summary = {\n",
    "    'cross_validation': {\n",
    "        'total_folds': total_folds,\n",
    "        'validation_metrics': {\n",
    "            'precision': {\n",
    "                'mean': float(avg_val_precision),\n",
    "                'std': float(std_val_precision)\n",
    "            },\n",
    "            'recall': {\n",
    "                'mean': float(avg_val_recall),\n",
    "                'std': float(std_val_recall)\n",
    "            },\n",
    "            'f1_score': {\n",
    "                'mean': float(avg_val_f1),\n",
    "                'std': float(std_val_f1)\n",
    "            }\n",
    "        },\n",
    "        'test_metrics': {\n",
    "            'precision': {\n",
    "                'mean': float(avg_test_precision),\n",
    "                'std': float(std_test_precision)\n",
    "            },\n",
    "            'recall': {\n",
    "                'mean': float(avg_test_recall),\n",
    "                'std': float(std_test_recall)\n",
    "            },\n",
    "            'f1_score': {\n",
    "                'mean': float(avg_test_f1),\n",
    "                'std': float(std_test_f1)\n",
    "            }\n",
    "        },\n",
    "        'best_overall_model': {\n",
    "            'model_path': best_overall_model_path,\n",
    "            'validation_f1_score': float(best_overall_f1),\n",
    "            'test_f1_score': float(best_overall_test_f1) if best_overall_test_f1 != -1 else None\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# print(\"\\nCross-Validation Metrics Summary:\")\n",
    "# print(json.dumps(metrics_summary, indent=4))\n",
    "# =============================================================\n",
    "\n",
    "# ================== Save Metrics to JSON =====================\n",
    "# Define the metrics file path\n",
    "metrics_file_path = os.path.join(metrics_dir, metrics_filename)\n",
    "\n",
    "# Save the metrics to a JSON file\n",
    "with open(metrics_file_path, 'w') as json_file:\n",
    "    json.dump(metrics_summary, json_file, indent=4)\n",
    "\n",
    "print(f\"\\nFinal aggregated metrics have been saved to '{metrics_file_path}'.\")\n",
    "print(f\"Best overall TFLite model saved at: '{best_overall_model_path}'\")\n",
    "# =============================================================\n",
    "\n",
    "# ================== Generate and Save Plots for Best Overall Model ============\n",
    "if best_overall_history is not None:\n",
    "    print(\"\\nGenerating plots for the best overall model...\")\n",
    "    \n",
    "    # Define plot filenames with fixed names\n",
    "    loss_plot_filename = \"Best_Model_Loss.png\"\n",
    "    precision_plot_filename = \"Best_Model_Precision.png\"\n",
    "    recall_plot_filename = \"Best_Model_Recall.png\"\n",
    "    f1_score_plot_filename = \"Best_Model_F1_Score.png\"\n",
    "    \n",
    "    # Plot Loss\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(best_overall_history.history['loss'], label='Training Loss')\n",
    "    plt.plot(best_overall_history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Best Model Loss Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(plots_dir, loss_plot_filename))\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot Precision\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(best_overall_history.history['Precision'], label='Training Precision')\n",
    "    plt.plot(best_overall_history.history['val_Precision'], label='Validation Precision')\n",
    "    plt.title('Best Model Precision Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(plots_dir, precision_plot_filename))\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot Recall\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(best_overall_history.history['Recall'], label='Training Recall')\n",
    "    plt.plot(best_overall_history.history['val_Recall'], label='Validation Recall')\n",
    "    plt.title('Best Model Recall Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(plots_dir, recall_plot_filename))\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot F1-Score\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(best_overall_history.history['F1_Score'], label='Training F1 Score')\n",
    "    plt.plot(best_overall_history.history['val_F1_Score'], label='Validation F1 Score')\n",
    "    plt.title('Best Model F1 Score Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(plots_dir, f1_score_plot_filename))\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Plots for the best overall model have been saved to '{plots_dir}'.\")\n",
    "else:\n",
    "    print(\"\\nNo best overall model was identified. Plot generation skipped.\")\n",
    "# ============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
